{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fine-tune the pretrained CHGNet for better accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from pymatgen.core import Structure\n",
    "from chgnet.model import CHGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 1. Prepare Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from chgnet.utils import read_json\n",
    "from pymatgen.core.structure import Structure\n",
    "\n",
    "sources = [\"chgnet_dataset_dhh_I-42d.json\", \"chgnet_dataset_dhh_with_stresses.json\"]\n",
    "\n",
    "merged = defaultdict(list)\n",
    "\n",
    "for s in sources:\n",
    "    d = read_json(s)\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, list):\n",
    "            merged[k].extend(v)\n",
    "        else:\n",
    "            merged[k].append(v)\n",
    "\n",
    "dataset_dict = dict(merged)\n",
    "\n",
    "structures = [Structure.from_dict(struct) for struct in dataset_dict[\"structure\"]]\n",
    "energies_per_atom = dataset_dict[\"energy_per_atom\"]\n",
    "forces = dataset_dict[\"force\"]\n",
    "stresses = dataset_dict.get(\"stress\") or None\n",
    "magmoms = None\n",
    "\n",
    "print(len(structures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Note that the stress output from CHGNet is in unit of GPa, here the -10 unit conversion\n",
    "modifies it to be kbar in VASP raw unit.\n",
    "If you're using stress labels from VASP, you don't need to do any unit conversions\n",
    "StructureData dataset class takes in VASP units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 2. Define DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chgnet.data.dataset import StructureData, get_train_val_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StructureData(\n",
    "    structures=structures,\n",
    "    energies=energies_per_atom,\n",
    "    forces=forces,\n",
    "    stresses=stresses,  # can be None\n",
    "    magmoms=magmoms,  # can be None\n",
    ")\n",
    "\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "    dataset, batch_size=8, train_ratio=0.8, val_ratio=0.1\n",
    ")\n",
    "\n",
    "train_idx = np.array(train_loader.sampler.indices)\n",
    "val_idx   = np.array(val_loader.sampler.indices)\n",
    "test_idx  = np.array(test_loader.sampler.indices)\n",
    "\n",
    "np.savez(\n",
    "    \"dhh_split_indices.npz\",\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "The training set is used to optimize the CHGNet through gradient descent, the validation set is used to see validation error at the end of each epoch, and the test set is used to see the final test error at the end of training. The test set can be optional.\n",
    "\n",
    "The `batch_size` is defined to be 8 for small GPU-memory. If > 10 GB memory is available, we highly recommend to increase `batch_size` for better speed.\n",
    "\n",
    "If you have very large numbers (>100K) of structures (which is typical for AIMD), putting them all in a python list can quickly run into memory issues. In this case we highly recommend you to pre-convert all the structures into graphs and save them as shown in `examples/make_graphs.py`. Then directly train CHGNet by loading the graphs from disk instead of memory using the `GraphData` class defined in `data/dataset.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 3. Define model and trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chgnet.model import CHGNet\n",
    "from chgnet.trainer import Trainer\n",
    "\n",
    "# Load pretrained CHGNet\n",
    "chgnet = CHGNet.load(model_name='r2scan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "It's optional to freeze the weights inside some layers. This is a common technique to retain the learned knowledge during fine-tuning in large pretrained neural networks. You can choose the layers you want to freeze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in chgnet.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for layer in [\n",
    "    chgnet.atom_embedding,\n",
    "    chgnet.bond_basis_expansion,\n",
    "    chgnet.bond_embedding,\n",
    "    chgnet.bond_weights_ag,\n",
    "    chgnet.bond_weights_bg,\n",
    "    chgnet.angle_basis_expansion,\n",
    "    chgnet.angle_embedding,\n",
    "    chgnet.bond_conv_layers,\n",
    "    chgnet.angle_layers,\n",
    "]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for conv in chgnet.atom_conv_layers[:-1]:\n",
    "    for param in conv.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for name, p in chgnet.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "total_trainable = sum(p.numel() for p in chgnet.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in chgnet.parameters())\n",
    "print(\"trainable:\", total_trainable, \"of\", total, f\"({100*total_trainable/total:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=chgnet,\n",
    "    targets=\"efs\",\n",
    "    optimizer=\"SGD\",\n",
    "    scheduler=\"CosLR\",\n",
    "    criterion=\"MSE\",\n",
    "    epochs=30,\n",
    "    learning_rate=5e-4,\n",
    "    use_device=\"cpu\",\n",
    "    print_freq=10,\n",
    "    allow_missing_labels = False,\n",
    "\n",
    "    energy_loss_ratio = 1,\n",
    "    force_loss_ratio = 1,\n",
    "    stress_loss_ratio = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 4. Start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "history = trainer.training_history \n",
    "\n",
    "history_json = json.dumps(history)\n",
    "with open(\"dhh_mae_history.json\", \"w\") as logs:\n",
    "    logs.write(history_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f0c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "targets = [\"e\", \"f\", \"s\"]\n",
    "titles  = {\"e\": \"Energy MAE\", \"f\": \"Forces MAE\", \"s\": \"Stresses MAE\"}\n",
    "\n",
    "n_epochs = len(history[targets[0]][\"train\"])\n",
    "epochs = np.arange(n_epochs)\n",
    "\n",
    "fig, axes = plt.subplots(len(targets), 1, figsize=(7, 3.6 * len(targets)), sharex=True)\n",
    "if len(targets) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, t in zip(axes, targets):\n",
    "    ax.plot(epochs, history[t][\"train\"], marker=\"o\", label=\"train\")\n",
    "    ax.plot(epochs, history[t][\"val\"],   marker=\"s\", label=\"val\")\n",
    "    ax.set_title(titles.get(t, f\"{t.upper()} MAE\"))\n",
    "    ax.set_ylabel(f\"{t.upper()} MAE\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "axes[-1].set_xlabel(\"Epoch\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"mae_all_targets_vs_epoch.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "After training, the trained model can be found in the directory of today's date. Or it can be accessed by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "best_model = trainer.best_model  # best model based on validation energy MAE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chgnet-venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
